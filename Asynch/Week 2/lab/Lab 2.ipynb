{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting file name\n",
    "emma = nltk.corpus.gutenberg.fileids()[0]\n",
    "#Retreiving emma content\n",
    "emma = nltk.corpus.gutenberg.raw(emma)\n",
    "#Converting to lowercase\n",
    "emma = emma.lower()\n",
    "#Tokenizing the emma words\n",
    "emmaWords = nltk.word_tokenize(emma)\n",
    "print(\"there are {:d} words in Emma\".format(len(emmaWords)))\n",
    "print(\"There are {:d} unique words in Emma\".format(len(set(emmaWords))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating frequency distribution\n",
    "emmaDict = FreqDist(emmaWords)\n",
    "#printing word and count in 30 most common words\n",
    "for word, count in emmaDict.most_common(30):\n",
    "    print(\"{:s} \\t {:d}\".format(word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Potentially processed?\n",
    "emmaWords2 = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "#lowercase of words\n",
    "emmaWords2 = [w.lower() for w in emmaWords2]\n",
    "#creating a new frequenct dictionary\n",
    "emmaDict2 = FreqDist(emmaWords2)\n",
    "#New word count\n",
    "for word, count in emmaDict2.most_common(30):\n",
    "    print(\"{:s} \\t {:d}\".format(word, count))\n",
    "#the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing without error\n",
    "emmaDict2.get(\"her\", \"Not found\")\n",
    "print(\"dave\" in emmaDict2)\n",
    "print(\"dave\" not in emmaDict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFinder(substring, wordlist):\n",
    "    results = []\n",
    "    for word in wordlist:\n",
    "        if substring in word:\n",
    "            results.append(word)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = wordFinder(\"zz\", emmaWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(\"^[^a-z]+$\")\n",
    "nonAlphaMatch = pattern.match(\"**\")\n",
    "if nonAlphaMatch: print(\"Non-Alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_filter(word):\n",
    "    pattern = re.compile(\"^[^a-z]+$\")\n",
    "    if pattern.match(word):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emmaWords2 = [word for word in emmaWords2 if not alpha_filter(word)]\n",
    "emmaDict2 = FreqDist(emmaWords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emmaDict2.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "emmaWords2 = [word for word in emmaWords2 if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emmaDict2 = FreqDist(emmaWords2)\n",
    "emmaDict2.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating bigrams\n",
    "emmaBigrams = list(nltk.bigrams(emmaWords2))\n",
    "bigramDict = FreqDist(emmaBigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramDict.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(emmaWords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_word_filter(alpha_filter)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored2 = finder.score_ngrams(bigram_measures.pmi)\n",
    "scored2[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(emmaWords)\n",
    "finder.apply_freq_filter(5)\n",
    "scores = finder.score_ngrams(bigram_measures.pmi)\n",
    "scores[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Lab\n",
    "---\n",
    "As demonstrated in the lab session:\n",
    "\n",
    "Choose a file that you want to work onâ€”either one of the files from the book corpus or one from the Gutenberg corpus.\n",
    "\n",
    "Make a bigram finder and experiment with whether to apply the filters or not. Run the scoring with both the raw frequency and the pmi scorers and compare results.\n",
    "\n",
    "To complete the exercise, choose one of your top 20 frequency lists to report to show to the class. Write an introductory sentence or paragraph telling what text you chose and what bigram filters and scorer you used. Put this and the frequency list in your response. You may check out the frequency lists of other corpora by other students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('samuel', 'enderby') \t 14.31\n",
      "('heidelburgh', 'tun') \t 13.77\n",
      "('father', 'mapple') \t 13.24\n",
      "('huzza', 'porpoise') \t 13.07\n",
      "('fiery', 'pit') \t 12.92\n",
      "('steering', 'oar') \t 12.07\n",
      "('slouched', 'hat') \t 11.99\n",
      "('centuries', 'ago') \t 11.82\n",
      "('cape', 'horn') \t 11.76\n",
      "('moby', 'dick') \t 11.58\n",
      "('seven', 'hundred') \t 11.52\n",
      "('new', 'york') \t 11.36\n",
      "('new', 'zealand') \t 11.36\n",
      "('new', 'bedford') \t 11.36\n",
      "('book', 'ii') \t 10.99\n",
      "('saturday', 'night') \t 10.76\n",
      "('drew', 'nigh') \t 10.66\n",
      "('english', 'whalers') \t 10.45\n",
      "('years', 'ago') \t 10.41\n",
      "('brought', 'alongside') \t 10.31\n"
     ]
    }
   ],
   "source": [
    "from nltk import *\n",
    "from nltk.collocations import *\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#Filter for only alpha words\n",
    "def alpha_filter(word):\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if pattern.match(word):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#List of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#List of moby dick words\n",
    "melville = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "melville = [w.lower() for w in melville]\n",
    "\n",
    "#Getting the different measures for bigram analysis\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures\n",
    "#finding bigrams from the corpus\n",
    "finder = BigramCollocationFinder.from_words(melville)\n",
    "#Requiring 5 instances of the bigram \n",
    "finder.apply_freq_filter(5)\n",
    "#Getting rid of non-alphabetical words\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "#Removing stopwords\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "#getting the mutual information scores\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "for bi, pmi in scored[:20]:\n",
    "    print('{0} \\t {1}'.format(bi, round(pmi, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored2 = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "scored2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
