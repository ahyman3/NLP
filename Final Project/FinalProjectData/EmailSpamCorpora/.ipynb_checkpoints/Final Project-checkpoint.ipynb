{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirPath = \"corpus/\"\n",
    "limitStr = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the limit argument from a string to an int\n",
    "limit = int(limitStr)\n",
    "\n",
    "# start lists for spam and ham email texts\n",
    "hamtexts = []\n",
    "spamtexts = []\n",
    "os.chdir(dirPath)\n",
    "# process all files in directory that end in .txt up to the limit\n",
    "#    assuming that the emails are sufficiently randomized\n",
    "for file in os.listdir(\"./spam\"):\n",
    "    if (file.endswith(\".txt\")) and (len(spamtexts) < limit):\n",
    "      # open file for reading and read entire file into a string\n",
    "      f = open(\"./spam/\"+file, 'r', encoding=\"latin-1\")\n",
    "      spamtexts.append (f.read())\n",
    "      f.close()\n",
    "for file in os.listdir(\"./ham\"):\n",
    "    if (file.endswith(\".txt\")) and (len(hamtexts) < limit):\n",
    "      # open file for reading and read entire file into a string\n",
    "      f = open(\"./ham/\"+file, 'r', encoding=\"latin-1\")\n",
    "      hamtexts.append (f.read())\n",
    "      f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam files: 1500\n",
      "Number of ham files: 1500\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of spam files:\",len(spamtexts))\n",
    "print (\"Number of ham files:\",len(hamtexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of mixed spam and ham email documents as (list of words, label)\n",
    "emaildocs = []\n",
    "# add all the spam\n",
    "for spam in spamtexts:\n",
    "    tokens = nltk.word_tokenize(spam)\n",
    "    emaildocs.append((tokens, 'spam'))\n",
    "# add all the regular emails\n",
    "for ham in hamtexts:\n",
    "    tokens = nltk.word_tokenize(ham)\n",
    "    emaildocs.append((tokens, 'ham'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Subject', ':', 'hplc', '-', 'four', 'square', '5', '/', '01', 'darren', ':', 'i', 'have', 'another', 'hplc', 'issue', '.', '.', '.', 'i', 'received', 'an', 'invoice', 'from', 'four', 'square', 'gas', 'company', 'today', 'for', 'may', '2001', 'gas', 'that', 'was', 'never', 'paid', '.', 'they', 'have', 'invoiced', '1', ',', '654', 'at', '4', '.', '825', '(', 'hsc', '-', '.', '085', ')', 'at', 'meter', '986887', '.', 'i', 'did', 'not', 'see', 'a', 'deal', 'in', 'unify', ',', 'so', 'i', 'went', 'to', 'pops', 'to', 'find', 'the', 'meter', '.', 'i', 'found', 'that', 'the', 'gas', 'was', 'pathed', 'to', 'the', 'strangers', 'gas', 'contract', '.', 'historically', ',', 'hplc', 'has', 'always', 'purchased', 'the', 'gas', 'at', 'this', 'meter', 'from', 'four', 'square', ',', 'but', 'it', 'doesn', \"'\", 't', 'look', 'like', 'a', 'deal', 'was', 'ever', 'entered', 'for', 'may', '.', 'the', 'april', 'deal', 'number', 'was', '695469', '.', 'did', 'hplc', 'purchase', 'this', 'gas', 'in', 'may', '?', 'megan'], 'ham')\n",
      "(['Subject', ':', 'fw', ':', 'free', 'digital', 'payperview', 'not', 'interested', '?'], 'spam')\n",
      "(['Subject', ':', 'hpl', 'noms', 'for', 'nov', '.', '30', ',', '2000', '(', 'see', 'attached', 'file', ':', 'hplnl', '130', '.', 'xls', ')', '-', 'hplnl', '130', '.', 'xls'], 'ham')\n",
      "(['Subject', ':', 'nomination', 'change', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'forwarded', 'by', 'ami', 'chokshi', '/', 'corp', '/', 'enron', 'on', '06', '/', '05', '/', '2000', '08', ':', '00', 'am', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', \"''\", 'jan', 'svajian', '``', 'on', '06', '/', '02', '/', '2000', '04', ':', '30', ':', '44', 'pm', 'to', ':', 'cc', ':', 'subject', ':', 'nomination', 'change', 'on', 'monday', 'june', '5', 'th', 'thru', 'june', '8', 'th', ',', 'the', 'quinduno', 'field', 'connected', 'to', 'the', 'oneok', 'stinnett', 'plant', 'will', 'be', 'shut', 'in', 'by', 'oneok', 'for', 'maintenance', '.', 'the', 'production', 'should', 'be', 'back', 'on', 'the', 'afternoon', 'of', 'june', '8', 'th', 'and', 'our', 'nomination', 'should', 'be', 'in', 'place', 'for', 'the', '9', 'th', '.', 'sorry', 'about', 'the', 'short', 'notice', 'but', 'i', 'just', 'confirmed', 'with', 'oneok', 'that', 'this', 'was', 'going', 'to', 'happen', '.', '-', '0600', 'rl', '.', 'xls'], 'ham')\n"
     ]
    }
   ],
   "source": [
    "# randomize the list\n",
    "random.shuffle(emaildocs)\n",
    "\n",
    "# print a few token lists\n",
    "for email in emaildocs[:4]:\n",
    "    print (email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the words\n",
    "all_words_list = [word for sent, cat in emaildocs for word in sent]\n",
    "#Creating a frequency list\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "#Top 2500 words\n",
    "word_items = all_words.most_common(1000)\n",
    "#Words only\n",
    "word_features = [word for word, count in word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 39009),\n",
       " ('.', 32909),\n",
       " (',', 23056),\n",
       " ('/', 20530),\n",
       " (':', 15990),\n",
       " ('the', 14690),\n",
       " ('to', 11216),\n",
       " ('and', 8119),\n",
       " ('of', 6786),\n",
       " ('a', 6229)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue as usual to get all words and create word features\n",
    "#word features only\n",
    "def get_word_features(document, word_features):\n",
    "    document_words = list(set(document))\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[\"V_{:s}\".format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('Label\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "## cross-validation ##\n",
    "# this function takes the number of folds, the feature sets\n",
    "# it iterates over the folds, using different sections for training and testing in turn\n",
    "#   it prints the precision, recall and F score for each fold \n",
    "#.  (it does not compute the average over the folds)\n",
    "def cross_validation_PRF(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round to produce the gold and predicted labels\n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier.classify(features))\n",
    "\n",
    "        # call the function with our data\n",
    "        eval_measures(goldlist, predictedlist)\n",
    "    # this version doesn't save measures and compute averages\n",
    "## cross-validation ##\n",
    "# this function takes the number of folds, the feature sets\n",
    "# it iterates over the folds, using different sections for training and testing in turn\n",
    "#   it prints the accuracy for each fold and the average accuracy at the end\n",
    "def cross_validation_accuracy(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)\n",
    "        print (i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "    # find mean accuracy over all rounds\n",
    "    print ('mean accuracy', sum(accuracy_list) / num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 300\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.927      0.993      0.959\n",
      "spam \t      0.993      0.931      0.961\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.911      1.000      0.954\n",
      "spam \t      1.000      0.910      0.953\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.844      1.000      0.915\n",
      "spam \t      1.000      0.859      0.924\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.896      1.000      0.945\n",
      "spam \t      1.000      0.912      0.954\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.878      1.000      0.935\n",
      "spam \t      1.000      0.883      0.938\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.926      1.000      0.962\n",
      "spam \t      1.000      0.920      0.958\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.893      1.000      0.943\n",
      "spam \t      1.000      0.904      0.950\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.942      0.992      0.966\n",
      "spam \t      0.994      0.953      0.973\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.926      1.000      0.962\n",
      "spam \t      1.000      0.932      0.965\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.879      1.000      0.936\n",
      "spam \t      1.000      0.903      0.949\n",
      "Each fold size: 300\n",
      "0 0.96\n",
      "1 0.9533333333333334\n",
      "2 0.92\n",
      "3 0.95\n",
      "4 0.9366666666666666\n",
      "5 0.96\n",
      "6 0.9466666666666667\n",
      "7 0.97\n",
      "8 0.9633333333333334\n",
      "9 0.9433333333333334\n",
      "mean accuracy 0.9503333333333333\n"
     ]
    }
   ],
   "source": [
    "# train classifier and show performance in cross-validation\n",
    "word_features_only = [(get_word_features(doc, word_features), cat) for doc, cat in emaildocs]\n",
    "#cross-validation\n",
    "cross_validation_PRF(10, word_features_only)\n",
    "cross_validation_accuracy(10, word_features_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "#Removing stop words\n",
    "no_stopwords_all = [word for sent, cat in emaildocs for word in sent if word not in stopwords]\n",
    "#removing punctuation and numbers\n",
    "pat = re.compile(\"[^A-Za-z]+\")\n",
    "only_words = [word for word in no_stopwords_all if re.match(pat, word) is None]\n",
    "#Putting in frequency distribution\n",
    "stopwords_removed_dist = nltk.FreqDist(only_words)\n",
    "#Getting top 1000 word features\n",
    "top_words_only = stopwords_removed_dist.most_common(1000)\n",
    "#Putting the words in a list\n",
    "word_features_stop_removed = [word for word, count in top_words_only]\n",
    "#Creating featureset\n",
    "no_stop_documents = [(get_word_features(doc, word_features_stop_removed),cat) for\n",
    "                     doc, cat in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 300\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.940      0.986      0.962\n",
      "spam \t      0.987      0.943      0.964\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.962      1.000      0.981\n",
      "spam \t      1.000      0.959      0.979\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.896      1.000      0.945\n",
      "spam \t      1.000      0.901      0.948\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.924      0.971      0.947\n",
      "spam \t      0.974      0.933      0.953\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.897      0.986      0.940\n",
      "spam \t      0.986      0.899      0.940\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.951      0.994      0.972\n",
      "spam \t      0.993      0.945      0.968\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.913      1.000      0.954\n",
      "spam \t      1.000      0.921      0.959\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.956      0.956      0.956\n",
      "spam \t      0.963      0.963      0.963\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.940      0.993      0.966\n",
      "spam \t      0.993      0.943      0.968\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.894      0.977      0.933\n",
      "spam \t      0.981      0.912      0.945\n",
      "Each fold size: 300\n",
      "0 0.9633333333333334\n",
      "1 0.98\n",
      "2 0.9466666666666667\n",
      "3 0.95\n",
      "4 0.94\n",
      "5 0.97\n",
      "6 0.9566666666666667\n",
      "7 0.96\n",
      "8 0.9666666666666667\n",
      "9 0.94\n",
      "mean accuracy 0.9573333333333333\n"
     ]
    }
   ],
   "source": [
    "#cross-validation\n",
    "cross_validation_PRF(10, no_stop_documents)\n",
    "cross_validation_accuracy(10, no_stop_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tag (with stopwords removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordfeatures_pos(document, word_features):\n",
    "    #regex pattern\n",
    "    all_caps = re.compile(\"[A-Z]+\")\n",
    "    #all words in the document\n",
    "    document_words = list(set(document))\n",
    "    #Features dictionary\n",
    "    features = {}\n",
    "    #Creating features with the word features\n",
    "    for word in word_features:\n",
    "        features[\"V_{:s}\".format(word)] = (word in document_words)\n",
    "    #initializing the pos count\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adj_count = 0\n",
    "    adv_count = 0\n",
    "    #Tagging the words\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    #If word falls in the noun, adj, adv, or verb category, add to count\n",
    "    for word, tag in tagged_words:\n",
    "        if tag.startswith(\"N\"): noun_count += 1\n",
    "        if tag.startswith(\"J\"): adj_count += 1\n",
    "        if tag.startswith(\"V\"): verb_count += 1\n",
    "        if tag.startswith(\"R\"): adv_count += 1\n",
    "    #Save count in the features dictionary\n",
    "    features[\"verbcount\"] = verb_count\n",
    "    features[\"adjcount\"] = adj_count\n",
    "    features[\"nouncount\"] = noun_count\n",
    "    features[\"advcount\"] = adv_count\n",
    "    return features\n",
    "#Creating the part of speech category dataset\n",
    "pos_features = [(get_wordfeatures_pos(doc, word_features_stop_removed), cat) for\n",
    "               doc, cat in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 300\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.940      0.972      0.956\n",
      "spam \t      0.973      0.942      0.957\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.968      0.994      0.981\n",
      "spam \t      0.993      0.966      0.979\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.883      0.965      0.922\n",
      "spam \t      0.966      0.887      0.925\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.910      0.942      0.926\n",
      "spam \t      0.949      0.919      0.934\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.904      0.986      0.943\n",
      "spam \t      0.986      0.904      0.944\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.944      0.981      0.962\n",
      "spam \t      0.978      0.938      0.957\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.913      0.978      0.944\n",
      "spam \t      0.980      0.919      0.949\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.956      0.949      0.953\n",
      "spam \t      0.957      0.963      0.960\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.933      0.979      0.955\n",
      "spam \t      0.980      0.937      0.958\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.894      0.962      0.926\n",
      "spam \t      0.969      0.911      0.939\n",
      "Each fold size: 300\n",
      "0 0.9566666666666667\n",
      "1 0.98\n",
      "2 0.9233333333333333\n",
      "3 0.93\n",
      "4 0.9433333333333334\n",
      "5 0.96\n",
      "6 0.9466666666666667\n",
      "7 0.9566666666666667\n",
      "8 0.9566666666666667\n",
      "9 0.9333333333333333\n",
      "mean accuracy 0.9486666666666667\n"
     ]
    }
   ],
   "source": [
    "#cross-validation for POS features added\n",
    "cross_validation_PRF(10, pos_features)\n",
    "cross_validation_accuracy(10, pos_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "#Removing stop words\n",
    "no_stopwords_all = [word for sent, cat in emaildocs for word in sent if word not in stopwords]\n",
    "#removing punctuation and numbers\n",
    "pat = re.compile(\"[^A-Za-z]+\")\n",
    "only_words = [word for word in no_stopwords_all if re.match(pat, word) is None]\n",
    "#Putting in frequency distribution\n",
    "stopwords_removed_dist = nltk.FreqDist(only_words)\n",
    "#Getting top 1000 word features\n",
    "top_words_only = stopwords_removed_dist.most_common(1000)\n",
    "top_words = [w for w, c in top_words_only]\n",
    "\n",
    "#Doc frequency\n",
    "doc_count = {}\n",
    "#Looping through each document\n",
    "for email, _ in emaildocs:\n",
    "    #getting a list of all unique words\n",
    "    list(set(email))\n",
    "    #Going through a list of only the top words\n",
    "    for word in top_words:\n",
    "        #If the word feature is in the document, add 1 to the value\n",
    "        doc_count[word] = doc_count.get(word, 0) + int(word in email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each word being used\n",
    "for w in doc_count.keys():\n",
    "    #Find the word in the dictionary, and change the present value to \n",
    "    #be the document count divided by 3000 \n",
    "    doc_count[w] = doc_count.get(w) / 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf(doc, docfreq_terms):\n",
    "    \"\"\"\n",
    "    This function takes in a document an a preprocessed document frequenct dictionary\n",
    "    and returns the tf * log10(1 / df) for each word in the word features\n",
    "    \"\"\"\n",
    "    #Creating a frequency distribution\n",
    "    termfreq = nltk.FreqDist(doc)\n",
    "    #looping through each unique word in the email document\n",
    "    for w in termfreq.keys():\n",
    "        #Getting the term frequency in the document\n",
    "        termfreq[w] = termfreq.get(w) / len(doc)\n",
    "    #Feature dictionary to be returned\n",
    "    features = {}\n",
    "    for w, df in docfreq_terms.items():\n",
    "        #Creates the feature dictionary to return\n",
    "        features[\"Tfidf_{:s}\".format(w)] = termfreq.get(w, 0) * np.log10(1 / df)\n",
    "    #return the feature dictionary\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = [(create_tfidf(doc, doc_count), cat) for doc, cat in emaildocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 300\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.864      0.834      0.849\n",
      "spam \t      0.850      0.877      0.863\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.887      0.892      0.890\n",
      "spam \t      0.864      0.857      0.860\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.776      0.832      0.803\n",
      "spam \t      0.850      0.798      0.823\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.881      0.876      0.879\n",
      "spam \t      0.857      0.863      0.860\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.821      0.862      0.841\n",
      "spam \t      0.877      0.840      0.858\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.901      0.789      0.841\n",
      "spam \t      0.786      0.899      0.839\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.869      0.906      0.887\n",
      "spam \t      0.916      0.882      0.899\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.812      0.893      0.850\n",
      "spam \t      0.897      0.819      0.856\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.849      0.849      0.849\n",
      "spam \t      0.845      0.845      0.845\n",
      "Label\tPrecision\tRecall\t\tF1\n",
      "ham \t      0.818      0.846      0.832\n",
      "spam \t      0.855      0.828      0.841\n",
      "Each fold size: 300\n",
      "0 0.8566666666666667\n",
      "1 0.8766666666666667\n",
      "2 0.8133333333333334\n",
      "3 0.87\n",
      "4 0.85\n",
      "5 0.84\n",
      "6 0.8933333333333333\n",
      "7 0.8533333333333334\n",
      "8 0.8466666666666667\n",
      "9 0.8366666666666667\n",
      "mean accuracy 0.8536666666666667\n"
     ]
    }
   ],
   "source": [
    "#cross-validation for tfidf features added\n",
    "cross_validation_PRF(10, tfidf_features)\n",
    "cross_validation_accuracy(10, tfidf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Creating tfidf vectorizer object\n",
    "tfidf = TfidfVectorizer(tokenizer= nltk.word_tokenize, stop_words= stopwords, lowercase = True,\n",
    "                       max_df = 0.9, max_features = 1000)\n",
    "\n",
    "random.seed(1000)\n",
    "#0 is normal text, 1 is spam\n",
    "spam_tuple = [(doc, 1) for doc in spamtexts]\n",
    "ham_tuple = [(doc, 0) for doc in hamtexts]\n",
    "#combining all texts\n",
    "all_texts = spam_tuple + ham_tuple\n",
    "#randomly shuffling all texts\n",
    "random.shuffle(all_texts)\n",
    "#text only\n",
    "email_text = [doc for doc, cat in all_texts]\n",
    "labels = [cat for doc, cat in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the tfidf based on all texts\n",
    "X = tfidf.fit_transform(email_text)\n",
    "#making the labels for y\n",
    "y = np.array(labels)\n",
    "y = y.reshape(3000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1, 5, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PArameter grid for finding the best fit on the train set\n",
    "param_grid = {\"C\":[1,5,10], \"kernel\": [\"linear\", \"rbf\", \"poly\"], \"degree\":[2,3]}\n",
    "#creating an svc model\n",
    "svc = SVC()\n",
    "#fitting the parameter grid to the model\n",
    "gs = GridSearchCV(svc, param_grid)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on hold out set: 0.9650\n"
     ]
    }
   ],
   "source": [
    "#Getting the accuracy on the test set\n",
    "print(\"Accuracy on hold out set: {:.4f}\".format(\n",
    "    accuracy_score(y_pred=gs.best_estimator_.predict(X_test), y_true=y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "#Accuracy of the model with a 10-fold crossvalidation on all data\n",
    "cv_results = cross_validate(gs.best_estimator_, X, y, cv = 10, scoring = (\"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy (10-fold): 0.9717\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Test Accuracy (10-fold): {:.4f}\".format(cv_results[\"test_score\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam f1 score: 0.9809\n",
      "Spam precision score: 0.9670\n",
      "Spam recall score: 0.9953\n",
      "\n",
      "Ham f1 score: 0.9804\n",
      "Ham precision score: 0.9952\n",
      "Ham recall score: 0.9660\n"
     ]
    }
   ],
   "source": [
    "predictions = gs.best_estimator_.predict(X)\n",
    "print(\"Spam f1 score: {:.4f}\".format(f1_score(y_pred=predictions, y_true=y, labels=[\"Ham\", \"Spam\"], pos_label=1)))\n",
    "print(\"Spam precision score: {:.4f}\".format(precision_score(y_pred=predictions, y_true=y, labels=[\"Ham\", \"Spam\"], pos_label=1)))\n",
    "print(\"Spam recall score: {:.4f}\\n\".format(recall_score(y_pred=predictions, y_true=y, labels=[\"Ham\", \"Spam\"], pos_label=1)))\n",
    "\n",
    "print(\"Ham f1 score: {:.4f}\".format(f1_score(y_pred=predictions, y_true=y, labels=[\"Ham\", \"Spam\"], pos_label=0)))\n",
    "print(\"Ham precision score: {:.4f}\".format(precision_score(y_pred=predictions, y_true=y, labels=[\"Ham\", \"Spam\"], pos_label=0)))\n",
    "print(\"Ham recall score: {:.4f}\".format(recall_score(y_pred=predictions, y_true=y, labels=[\"Ham\", \"Spam\"], pos_label=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=2, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
